# -*- coding: utf-8 -*-
"""GNR638_Project_ALAE.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ptbNEmab42bYRL7x5t2l89XOftW87VFZ

# **Importing Libraries**
"""

!pip install tensorflow_addons

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import os
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import torchvision.datasets as datasets
import torch.nn.parallel
import torch.optim as optim
from torch.autograd import Variable
import torchvision.utils as vutils
import time
from tqdm import tqdm_notebook as tqdm
from torchvision.utils import save_image
from IPython.display import clear_output
import tensorflow_addons as tfa

# %matplotlib inline

"""# **Collecting Dataset**

"""

from google.colab import drive
drive.mount('/content/drive')

from torch.utils.data import DataLoader
from torchvision.datasets import ImageFolder
import torchvision.transforms as T

directory = '/content/drive/MyDrive/Anime dataset/images/'

images = os.listdir(directory)
print(f'There are {len(os.listdir(directory))} images.')

fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(17,10))
for indx, axis in enumerate(axes.flatten()):
  index = np.random.randint(0, len(os.listdir(directory)))
  img_read = plt.imread(directory + images[index])
  imgplot = axis.imshow(img_read)
  axis.set_title(images[index])
  axis.set_axis_off()
plt.tight_layout(rect=[0, 0.05, 1, 0.90])

"""Reference: https://matplotlib.org/users/image_tutorial.html"""

batch_size = 32
batchSize = 64
imageSize = 64
transform = T.Compose([T.Resize(64),
                                T.CenterCrop(64),
                                T.ToTensor(),
                                T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])

train_data = datasets.ImageFolder('/content/drive/MyDrive/Anime dataset/', transform=transform)

dataloader = torch.utils.data.DataLoader(train_data, shuffle=True,
                                           batch_size=batch_size)

imgs, label = next(iter(dataloader))
imgs = imgs.numpy().transpose(0, 2, 3, 1)

"""New data Data loader and Augmentations from RaLSGAN dogs"""

batch_size = 32
image_size = 64

random_transforms = [T.ColorJitter(), T.RandomRotation(degrees=20)]
transform = T.Compose([T.Resize(64),
                                T.CenterCrop(64),
                                T.RandomHorizontalFlip(p=0.5),
                                T.RandomApply(random_transforms, p=0.2),
                                T.ToTensor(),
                                T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])

train_data = datasets.ImageFolder('/content/drive/MyDrive/Anime dataset/', transform=transform)
train_loader = torch.utils.data.DataLoader(train_data, shuffle=True,
                                           batch_size=batch_size)

imgs, label = next(iter(train_loader))
imgs = imgs.numpy().transpose(0, 2, 3, 1)

for i in range(5):
    plt.imshow(imgs[i])
    plt.show()

"""# **Defining functions**

# **Weights Initialization**
"""

def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Conv') != -1:
        m.weight.data.normal_(0.0, 0.02)
    elif classname.find('BatchNorm') != -1:
        m.weight.data.normal_(1.0, 0.02)
        m.bias.data.fill_(0)

"""# **Standard Generator**

"""

class Stand_Gen2(nn.Module):
    def __init__(self, nz=128, channels=3):
        super(Stand_Gen2, self).__init__()

        self.nz = nz
        self.channels = channels

        def convlayer(n_input, n_output, k_size=4, stride=2, padding=0):
            block = [
                nn.ConvTranspose2d(n_input, n_output, kernel_size=k_size, stride=stride, padding=padding, bias=False),
                nn.BatchNorm2d(n_output),
                nn.ReLU(inplace=True),
            ]
            return block

        self.model = nn.Sequential(
            *convlayer(self.nz, 1024, 4, 1, 0),
            *convlayer(1024, 512, 4, 2, 1),
            *convlayer(512, 256, 4, 2, 1),
            *convlayer(256, 128, 4, 2, 1),
            *convlayer(128, 64, 4, 2, 1),
            nn.ConvTranspose2d(64, self.channels, 3, 1, 1),
            nn.Tanh()
        )

    def forward(self, z):
        z = z.view(-1, self.nz, 1, 1)
        img = self.model(z)
        return img

"""# **Standard Discriminator**"""

class Stand_Disc2(nn.Module):
    def __init__(self, channels=3):
        super().__init__()
        self.channels = channels
        def convlayer(n_input, n_output, k_size=4, stride=2, padding=0, bn=False):
            block = [nn.Conv2d(n_input, n_output, kernel_size=k_size, stride=stride, padding=padding, bias=False)]
            if bn:
                block.append(nn.BatchNorm2d(n_output))
            block.append(nn.LeakyReLU(0.2, inplace=True))
            return block
        self.model = nn.Sequential(
            *convlayer(self.channels, 32, 4, 2, 1),
            *convlayer(32, 64, 4, 2, 1),
            *convlayer(64, 128, 4, 2, 1, bn=True),
            *convlayer(128, 256, 4, 2, 1, bn=True),
            nn.Conv2d(256, 1, 4, 1, 0, bias=False),
        )
    def forward(self, imgs):
        logits = self.model(imgs)
        out = torch.sigmoid(logits)
        return out.view(-1, 1)

"""# **Training model**

# **Parameter tuning of standard GAN**
"""

batch_size = 32
LR_G = 0.001
LR_D = 0.0005
beta1 = 0.5
epochs = 10
real_label = 0.9
fake_label = 0
nz = 128
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

"""# **Initializing the optimizers and the models**"""

netG2 = Stand_Gen2(nz).to(device)
netD2 = Stand_Disc2().to(device)
criterion = nn.BCELoss()
optimizerD = optim.Adam(netD2.parameters(), lr=LR_D, betas=(beta1, 0.999))
optimizerG = optim.Adam(netG2.parameters(), lr=LR_G, betas=(beta1, 0.999))
fixed_noise = torch.randn(25, nz, 1, 1, device=device)

G_losses = []
D_losses = []
epoch_time = []

"""# **Plot function for loss per epoch**"""

def plot_loss (G_losses, D_losses, epoch):
    plt.figure(figsize=(15,10))
    plt.title("Generator and Discriminator Loss - EPOCH "+ str(epoch))
    plt.plot(G_losses,label="G")
    plt.plot(D_losses,label="D")
    plt.xlabel("iterations")
    plt.ylabel("Loss")
    plt.legend()
    plt.show()

"""# **Image generating function**"""

def show_generated_images(n_images=5):
    sample = []
    for _ in range(n_images):
        noise = torch.randn(1, nz, 1, 1, device=device)
        gen_image = netG2(noise).to("cpu").clone().detach().squeeze(0)
        gen_image = gen_image.numpy().transpose(1, 2, 0)
        sample.append(gen_image)
    figure, axes = plt.subplots(1, len(sample), figsize = (64,64))
    for index, axis in enumerate(axes):
        axis.axis('off')
        image_array = sample[index]
        axis.imshow(image_array)
    plt.show()
    plt.close()

"""# **Running epochs on Standard GAN**"""

# Commented out IPython magic to ensure Python compatibility.
for epoch in range(epochs):
    start = time.time()
    for ii, (real_images, train_labels) in tqdm(enumerate(train_loader), total=len(train_loader)):
        #### Training with the real data
        netD2.zero_grad()
        real_images = real_images.to(device)
        batch_size = real_images.size(0)
        labels = torch.full((batch_size, 1), real_label, device=device)
        output = netD2(real_images)
        errD_real = criterion(output, labels)
        errD_real.backward()
        D_x = output.mean().item()
        #### Trianing with the fake data
        noise = torch.randn(batch_size, nz, 1, 1, device=device)
        fake = netG2(noise)
        labels.fill_(fake_label)
        output = netD2(fake.detach())
        errD_fake = criterion(output, labels)
        errD_fake.backward()
        D_G_z1 = output.mean().item()
        errD = errD_real + errD_fake
        optimizerD.step()
        #### We maximize log(D(G(z)))
        netG2.zero_grad()
        labels.fill_(real_label)
        output = netD2(fake)
        errG = criterion(output, labels)
        errG.backward()
        D_G_z2 = output.mean().item()
        optimizerG.step()
        # We save the respective losses for plotting later
        G_losses.append(errG.item())
        D_losses.append(errD.item())
        if (ii+1) % (len(train_loader)//2) == 0:
            print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f'
#                   % (epoch + 1, epochs, ii+1, len(train_loader),
                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))
    plot_loss (G_losses, D_losses, epoch)
    G_losses = []
    D_losses = []
    if epoch % 10 == 0:
        show_generated_images()
    epoch_time.append(time.time()- start)

print ("Average EPOCH duration= ", np.mean(epoch_time))

if not os.path.exists('../output_images'):
    os.mkdir('../output_images')
im_batch_size = 50
n_images=10000
for i_batch in tqdm(range(0, n_images, im_batch_size)):
    gen_z = torch.randn(im_batch_size, nz, 1, 1, device=device)
    gen_images = netG2(gen_z)
    images = gen_images.to("cpu").clone().detach()
    images = images.numpy().transpose(0, 2, 3, 1)
    for i_image in range(gen_images.size(0)):
        save_image(gen_images[i_image, :, :, :], os.path.join('../output_images', f'image_{i_batch+i_image:05d}.png'))

fig = plt.figure(figsize=(28, 20))
for i, j in enumerate(images[:32]):
    ax = fig.add_subplot(4, 8, i + 1, xticks=[], yticks=[])
    plt.imshow(j)

"""# **Saving our StandardGAN models**"""

torch.save(netG2.state_dict(), 'standard_generator.pth')
torch.save(netD2.state_dict(), 'standard_discriminator.pth')

"""# **Our Implementation from the research paper**"""

!pip install tensorflow_addons

import tensorflow_addons as tfa

"""# **Loading our dataset**"""

anime_img_path =  '/content/drive/MyDrive/Anime dataset/images'
anime_imgs = np.array(sorted(os.listdir(anime_img_path)))

anime_imgs_1 = list(anime_imgs[[0,52,19,54,39,71]])
anime_imgs_2 = list(anime_imgs[[4,43,55,45,31,23]])

ImageSet_1 = [(anime_img_path)+'/%s'%(x) for x in anime_imgs_1]
ImageSet_2 = [(anime_img_path)+'/%s'%(x) for x in anime_imgs_2]

def load(image_file):
    image = tf.io.read_file(image_file)
    image = tf.image.decode_jpeg(image)
    image_ = tf.cast(image, tf.float32)
    return image_

"""# **Resize function**"""

def resize(input_, height, width):
    img_ = tf.image.resize(input_, [height, width],
                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)
    return img_

"""# **Normalizing the images in the range [-1,1]**"""

def normalize(input_):
    image = tf.cast(input_, tf.float32)
    img_ = (image / 127.5) - 1
    return img_

"""# **Function to introduce random jitters in Images**"""

@tf.function()
def random_jitter(input_):
    img_ = resize(input_,256, 256)

    if tf.random.uniform(()) > 0.5:
        img_ = tf.image.flip_left_right(img_)

    return img_

def load_image_train(file_):
    input_ = load(file_)
    img_ = random_jitter(input_)
    img_ = normalize(img_)

    return img_

AUTOTUNE = tf.data.experimental.AUTOTUNE

"""# **Defining the source and the destination training dataset**"""

train_source = (
    tf.data.Dataset
    .from_tensor_slices((ImageSet_1))
    .map(load_image_train, num_parallel_calls=AUTOTUNE)
    .shuffle(7)
    .batch(32)
)
train_dest = (
    tf.data.Dataset
    .from_tensor_slices((ImageSet_2))
    .map(load_image_train, num_parallel_calls=AUTOTUNE)
    .shuffle(7)
    .batch(32)
)

"""# **Function to add noise**"""

def noise_(num, filter, noise):
    noise_layer = tf.keras.layers.Dense(num*num*filter)(noise)
    reshape_noise = tf.keras.layers.Reshape((num, num, filter))(noise_layer)
    return reshape_noise

"""# **Function for downsampling**"""

def downsample(filters, size, stride, apply_batchnorm=True):
    initializer = tf.random_normal_initializer(0., 0.02)
    result = tf.keras.Sequential()
    result.add(
              tf.keras.layers.Conv2D(
                  filters,
                  size,
                  strides=stride,
                  padding='same',
                  kernel_initializer=initializer,
                  use_bias=False)
    )
    if apply_batchnorm:
        result.add(tfa.layers.InstanceNormalization())

    result.add(tf.keras.layers.LeakyReLU(0.4))

    return result

"""# **Function for upsampling**"""

def upsample(filters, size, stride):
    initializer = tf.random_normal_initializer(0., 0.02)
    result = tf.keras.Sequential()
    layer = tf.keras.layers.Conv2DTranspose(
            filters,
            size,
            strides=stride,
            padding='same',
            kernel_initializer=initializer,
            use_bias=False)

    result.add(
        layer
    )
    return result

"""# **Generator**"""

def Generator():
    input_encoder = tf.keras.layers.Input(shape=[256, 256,3])
    input_generator = tf.keras.layers.Input(shape=[100, ])
    initializer = tf.random_normal_initializer(0., 0.02)

    down_stack = [downsample(64, 3, 2,False),
        downsample(128, 3, 2),
        downsample(256, 3, 2,False),
        downsample(512, 3, 2,),
        downsample(512, 3, 2,),
        downsample(512, 3, 2,False),
     ]
    x = input_encoder
    skips = []
    for down in down_stack:
        x = down(x)
        skips.append(x)

    skips = reversed(skips[:-1])
    gen_layer = tf.keras.layers.Dense(4*4*512)(input_generator)
    gen_layer = tf.keras.layers.BatchNormalization()(gen_layer)
    gen_layer = tf.keras.layers.LeakyReLU(0.4)(gen_layer)

    reshape = tf.keras.layers.Reshape((4, 4, 512))(gen_layer)
    x = reshape
    up_stack = [upsample(512, 3, 2, ),
              upsample(512, 3, 2, ),
              upsample(256, 3, 2, ),
              upsample(128, 3, 2, ),
              upsample(64, 3, 2, ), ]
    last = tf.keras.layers.Conv2DTranspose(3, 3,
                                      strides=2,
                                      padding='same',
                                      kernel_initializer=initializer,
                                      activation='tanh')
    filters = [512,512,256,128,64]
    ndim = [8,16,32,64,128]
    for up, skip, dim, filt in zip(up_stack, skips, ndim, filters):
        x = up(x)
        n = noise_(dim, filt, input_generator)
        x = tf.keras.layers.concatenate([n, x])
        x = tf.keras.layers.BatchNormalization()(x)
        x = tf.keras.layers.LeakyReLU(0.4)(x)

        x = tf.keras.layers.concatenate([x, skip])
    x = last(x)
    return tf.keras.Model(inputs=[input_encoder,input_generator], outputs=x)

generator = Generator()
generator.summary()

"""# **Discriminator**"""

def Discriminator():
    initializer = tf.random_normal_initializer(0., 0.02)
    input_ = tf.keras.layers.Input(shape=[256, 256, 3], name='input_image')
    down1 = downsample(64, 3, 2,False)(input_)
    down2 = downsample(128, 3, 2)(down1)

    zero_pad1 = tf.keras.layers.ZeroPadding2D()(down2)
    conv = tf.keras.layers.Conv2D(256, 3, strides=1,
                                kernel_initializer=initializer,
                                use_bias=False)(zero_pad1)
    batchnorm1 = tf.keras.layers.BatchNormalization()(conv) #Itfa.layers.InstanceNormalization()

    leaky_relu = tf.keras.layers.LeakyReLU(0.4)(batchnorm1)

    zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu)

    last = tf.keras.layers.Conv2D(1, 3, strides=1,
                                kernel_initializer=initializer,
                                )(zero_pad2)
    return tf.keras.Model(inputs=input_, outputs=last)

discriminator = Discriminator()
tf.keras.utils.plot_model(discriminator, show_shapes=True, dpi=64)

"""# **Defining the Loss functions**"""

LAMBDA = 10
loss_obj = tf.keras.losses.BinaryCrossentropy(from_logits=True)

"""# **Loss for Discriminator**"""

def discriminator_loss(real, generated):
    real_loss = loss_obj(tf.ones_like(real), real)

    generated_loss = loss_obj(tf.zeros_like(generated), generated)
    total_disc_loss = real_loss + (generated_loss)*LAMBDA*0.5
    loss_ = (generated_loss)*LAMBDA*0.5

    return total_disc_loss, loss_

"""# **Loss for Generator**"""

def generator_loss(disc_generated_output, gen_output, target):
    gan_loss = loss_obj(tf.ones_like(disc_generated_output), disc_generated_output)
    ### We define the MAE
    l1_loss = tf.reduce_mean(tf.abs(target - gen_output))
    total_gen_loss = gan_loss + (LAMBDA *(l1_loss)*0.5)
    return total_gen_loss, gan_loss, l1_loss

def reconstruction(recon_x, x):
    return tf.reduce_mean(tf.abs(recon_x - x))

"""# **Optimizers**"""

generator_optimizer = tf.keras.optimizers.Adam(1e-4, beta_1=0.5, beta_2=0.99,epsilon=1e-08)
discriminator_optimizer = tf.keras.optimizers.Adam(1e-4, beta_1=0.5, beta_2=0.99,epsilon=1e-08)

"""# **Image Generation**"""

def generate_images(model, input_1, input_2, noise):
    prediction = model([input_1, noise], training=True)
    plt.figure(figsize=(15,15))

    display_list = [input_1[0], input_2[0], prediction[0]]
    title = ['Source Image', 'Destination Image', 'Predicted Image']

    for i in range(3):
        plt.subplot(1, 4, i+1)
        plt.title(title[i])
        # getting the pixel values between [0, 1] to plot it.
        plt.imshow(display_list[i] * 0.5 + 0.5)
        plt.axis('off')
    plt.show()

example_input_1 = next(iter(train_source))
example_input_2 = next(iter(train_dest))
noise = tf.random.normal([6, 100])
generate_images(generator, example_input_1, example_input_2,noise)

@tf.function
def train_step(real_x, real_y, noise):
    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:
        generated_images = generator([real_x,noise], training=True)
        real_output = discriminator(real_y, training=True)
        fake_output = discriminator(generated_images, training=True)
        gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss(fake_output, generated_images, real_y)
        recon_loss = reconstruction(generated_images, real_x)
        disc_loss, _ = discriminator_loss(real_output, fake_output)
    gradients_of_generator = gen_tape.gradient(gen_total_loss, generator.trainable_variables)
    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)
    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))
    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))

result=[]
for epoch in range(500):
    start = time.time()
    n = 0
    for image_x, image_y in tf.data.Dataset.zip((train_source, train_dest)):
        noise = tf.random.normal([6, 100])
        train_step(image_x, image_y, noise)
        if n % 20 == 0:
            print ('.', end='')
        n+=1
    clear_output(wait=True)
    generate_images(generator, image_x, image_y, noise)
    print ('The time taken for epoch {} is {} sec\n'.format(epoch + 1,                                                      time.time()-start))
    if epoch>=300:
        pred = generator([image_x, noise], training=True)
        result.append(pred[0])

"""# **The Three sets**

# **The source set**
"""

source = []
dest = []
for x,y in zip(ImageSet_1,ImageSet_2):
    source.append(load_image_train(x))
    dest.append(load_image_train(y))
fig, axis = plt.subplots(2, 3, figsize=(18, 10))
for i, ax in enumerate(axis.flat):
    ax.imshow(source[i] * 0.5 + 0.5)
    ax.set(title = f"Source Set")
    ax.axis('off')

"""# **The destination set**"""

fig, axis = plt.subplots(2, 3, figsize=(18, 10))
for i, ax in enumerate(axis.flat):
    ax.imshow(dest[i] * 0.5 + 0.5)
    ax.set(title = f"Destination Set")
    ax.axis('off')

"""# **The Generated set**"""

res_1 = result[:15]
fig, axis = plt.subplots(5, 3, figsize=(25, 40))
for i, ax in enumerate(axis.flat):
    ax.imshow(res_1[i] * 0.5 + 0.5)
    ax.axis('off')

res_2 = result[15:30]
fig, axis = plt.subplots(5, 3, figsize=(25, 40))
for i, ax in enumerate(axis.flat):
    ax.imshow(res_2[i] * 0.5 + 0.5)
    ax.axis('off')

res_3 = result[30:45]
fig, axis = plt.subplots(5, 3, figsize=(25, 40))
for i, ax in enumerate(axis.flat):
    ax.imshow(res_3[i] * 0.5 + 0.5)
    ax.axis('off')

res_4 = result[45:60]
fig, axis = plt.subplots(5, 3, figsize=(25, 40))
for i, ax in enumerate(axis.flat):
    ax.imshow(res_4[i] * 0.5 + 0.5)
    ax.axis('off')

res_5 = result[60:75]
fig, axis = plt.subplots(5, 3, figsize=(25, 40))
for i, ax in enumerate(axis.flat):
    ax.imshow(res_5[i] * 0.5 + 0.5)
    ax.axis('off')

res_6 = result[75:90]
fig, axis = plt.subplots(5, 3, figsize=(25, 40))
for i, ax in enumerate(axis.flat):
    ax.imshow(res_6[i] * 0.5 + 0.5)
    ax.axis('off')

res_7 = result[90:105]
fig, axis = plt.subplots(5, 3, figsize=(25, 40))
for i, ax in enumerate(axis.flat):
    ax.imshow(res_7[i] * 0.5 + 0.5)
    ax.axis('off')







